---
title: 'Lab Assignment #4'
author: "Math 437 - Modern Data Analysis"
date: "Due February 27, 2023"
output: html_document
---

# Instructions

The purpose of this lab is to review simple linear regression and multiple linear regression strategies from Math 338/439.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

In this lab, we will be working with the Boston housing dataset (`Boston` in the `ISLR2` library). This dataset has 506 rows and 13 variables.

```{r libraries and data, message = FALSE, warning = FALSE}
library(ISLR2)
library(ggplot2)
library(dplyr)
library(car) # For problem 3
```

This lab assignment is worth a total of **19.5 points**.

# Problem 1: Bootstrap Estimation of Standard Error

## Part a (Code: 0.5 pts)

Run the code in the first half of ISLR Lab 5.3.4, "Estimating the Accuracy of a Statistic of Interest." Put each chunk from the textbook in its own chunk.

If you are in the actuarial science concentration, you should be familiar with (or will at some point see) this formula! For the rest of us, note that $X$ and $Y$ are assumed to be the yearly return of two different financial assets, and $\alpha$, the quantity to be estimated, is the fraction of money to be invested in $X$ such that the variance (risk) of the total investment $\alpha X + (1 - \alpha) Y$ is minimized. In this problem $\alpha$ is not the significance level!

```{r}
alpha.fn <- function (data , index) {
  X <- data$X[index]
  Y <- data$Y[index]
  ( var (Y) - cov (X, Y)) / ( var (X) + var (Y) - 2 * cov (X, Y))
}
```

```{r}
alpha.fn(Portfolio, 1:100)
```
```{r}
set.seed(7)
alpha.fn(Portfolio, sample(100, 100, replace = T))
```
```{r}
#install.packages("boot")
#library(boot)
boot(Portfolio, alpha.fn, R = 1000)
```

## Part b (Code: 2 pts)

According to the instructions for Lab 5.3.4, "We can implement a bootstrap analysis by performing this command [alpha.fn on a bootstrap sample] many times, recording all of the corresponding estimates for $\alpha$, and computing the resulting standard deviation."

Write a code chunk that performs all of those steps and prints out the standard deviation. Use 1000 bootstrap samples. 
```{r}
# 1. get a bootstrap sample
## Use sample()
#bootSample <- sample[,], replace = T)
# 2. Call alpha.fn on bootstrap sample and store the output (alpha)
## use alpha.fn()
#alpha.fn(Portfolio, )
# 3. Repeat Steps 1 and 2 many times (to get many estimates of alpha)
## use for loop
# 4. Get SD of the alpha-estimates
## Use sd()
set.seed(7)
BS <- matrix(nrow=1,ncol=1000)
for (i in 1:1000) {
  BS[i] = alpha.fn(Portfolio, sample(100, 100, replace = T))
}
sd(BS)
```


## Part c (Code: 1 pt)

Replicate the center panel of textbook Figure 5.10: a histogram of the bootstrap estimates of $\alpha$ (from Part b) with a solid pink (or red) line at the true value of $\alpha = 0.6$. You may use either base R plotting commands (which uses `abline` to add the vertical line) or the `ggplot2` package (which adds a `geom_vline` to the plot). 

## Part d (Explanation: 1 pt)

Note that the distribution you graphed in Part c is a sampling distribution of $\hat{\alpha}$. Explain why it would be appropriate to use this sampling distribution to construct a confidence interval for $\alpha$, but not to obtain a p-value for a hypothesis test of $H_0: \alpha = 0.6$ against $H_a: \alpha \neq 0.6$.

since we bootstrapped, we can't get a pvalue for alpha.
when we bootstrapped, we didn't know $\mu$, we don't need to know alpha. 

# Problem 2: Domain Knowledge and Exploratory Data Analysis

## Part a (Explanation: 1 pt)

Do an Internet search for "Boston housing dataset" and answer the following questions as best you can.

* Who collected this data? How old is this dataset?

The data was collected by the U.S. Census Service concerning housing in the area of Boston, Massachusetts. This data is from the 1970 census, so about 53 years old. 

* What does one row in this dataset represent?
One row in this dataset represents housing in a Boston suburb or town. 

## Part b (Explanation: 1.5 pts)

In your search, you should eventually come across references to a *fourteenth* variable, `B`, which the textbook authors have removed from the dataset. What does this mysterious variable represent?

The 14th variable `B` is $1000(Bk - 0.63)^2$ where Bk is the proportion of blacks(sic) by town. 

Suppose you are a data scientist at Zillow or a similar company whose housing price models are often used as a reference when people decide how much to offer to buy or sell a home for. What ethical issues would arise from using the variable `B` in your model?

The original description of the variable `B` might be offensive since it is *the proportion of blacks by town*. It's also unethical to use race as a predictor for housing price. 

## Part c (Code: 1 pt; Explanation: 1 pt)

In the next problem we will be trying to predict `medv` from `lstat`. What does the variable `medv` represent? What are the measurement units?

`medv` represents the median value of owner-occupied homes in $1000's (thousands of dollars).

Using the `ggplot2` package, create a histogram of the variable `medv`. Use a `center` of 35 and a `binwidth` of 2.

```{r}
ggplot(data=Boston, aes(x=medv)) + geom_histogram(center=35, binwidth=2)
```


What looks a bit off about this histogram? Try filling in the `filter` function in the chunk below to confirm your suspicions.
The histogram roughly follows a normal distribution but towards the very right side, there are about 20 observations with medv=$50K when there shouldn't be that many observations.  
```{r filter Boston medv, eval = FALSE}
Boston %>% 
  filter(medv > 48) %>%
  count() # getting sample size without having to summarize
```

## Part d (Code: 1 pt; Explanation: 1 pt)

The full documentation for this dataset is somewhat confusing and raises more questions than answers. For example, `lstat` is defined as "$\frac{1}{2}$ (proportion of adults without some high school education and proportion of male workers classified as laborers)" (whatever that means), and `rad` represents the "index of accessibility to radial highways" as determined by something called the "MIT Boston Project."

Other variables are sensibly defined, but are counterintuitive to what we would expect. Pick either the variable `age` or `rm`, and answer the following questions:

* What do you expect this variable would represent, if the observational units were houses?
I expect `age` to represent the average age of the house. 
* What does this variable actually represent?
`age` actually represents the proportion of owner-occupied units built prior to 1940. 
* What is the distribution of this variable in the dataset? Include at least one graph to support your answer.
The distribution of `age` is left-skewed. There are many houses old houses and not as many new houses. 
```{r}
ggplot(data=Boston, aes(x=age)) + geom_histogram(binwidth=8)
```

* Does this variable appear to have a relationship with the response variable `medv`? Include at least one graph to support your answer.
```{r}
ggplot(data=Boston, aes(x=age, y=medv)) + geom_point()
```


# Problem 3: Simple Linear Regression

## Part a (Code: 0.5 pts; Explanation: 1 pt)

Run the code in ISLR Lab 3.6.2. Put each chunk from the textbook in its own chunk.

Briefly explain what the `confint()` and `predict()` functions output when applied to a linear model.
```{r}
head(Boston)
```

```{r eval=FALSE}
?Boston
```

```{r eval=FALSE}
lm.fit <- lm(medv~lstat)
```

```{r}
lm.fit <- lm(medv ~ lstat, data = Boston)
attach(Boston)
lm.fit <- lm(medv ~ lstat)
```

```{r}
lm.fit
summary(lm.fit)
```

```{r}
names(lm.fit)
#we can extract quantities by name (e.g. lm.fit$coefficients) but it is safer to use extractor function like coef() to access them
coef(lm.fit)
```

```{r}
#Confidence intervals for coefficient estimates
confint(lm.fit)
```

```{r}
#95% Confidence interval for the prediction of medv for a given value of lstat
predict(lm.fit, data.frame(lstat = (c(5, 10, 15))), interval = "confidence")
```

```{r}
#95% Prediction interval for the prediction of medv for a given value of lstat
#prediction interval is wider than confidence interval
predict(lm.fit, data.frame(lstat = (c(5, 10, 15))), interval = "prediction")
```

```{r}
plot(lstat, medv)
abline(lm.fit)
```

```{r additional setting for plotting lines and points}
plot(lstat, medv)
abline(lm.fit, lwd = 3) #can be used to draw any line, not just LS regression line
abline(lm.fit, lwd = 3, col = "red")
plot(lstat, medv, col = "red")
plot(lstat, medv, pch = 20)
plot(lstat, medv, pch = "+")
plot(1:20, 1:20, pch = 1:20)
```

```{r}
par(mfrow = c(2,2))
plot(lm.fit)
```

```{r}
plot(predict(lm.fit), residuals(lm.fit))
plot(predict(lm.fit), rstudent(lm.fit))
```

```{r}
plot(hatvalues(lm.fit))
which.max(hatvalues(lm.fit))
```

## Part b (Explanation: 2.5 pts)

Write out the equation of the least-squares line relating `lstat` and `medv`. Write two sentences interpreting the parameter estimates (one for slope, one for intercept) in the context of the data. Remember to use the right observational units!


Given the issue raised in Problem 1d with the `lstat` interpretation, let's just say in our interpretations that `lstat` represents the percentage of people in the neighborhood considered lower class.

## Part c (Explanation: 1.5 pts)

Refer to the diagnostic plots you created in Part (a) to answer the following questions:

* Why do the lab instructions claim that "there is some evidence of non-linearity"?
* Do you believe that the residuals are normally distributed? Why or why not?
* Do you believe that the response variable is homoskedastic (the residuals have roughly constant variance across the entire predictor range)? Why or why not?

# Problem 4: Multiple Linear Regression

## Part a (Code: 0.5 pts; Explanation: 1 pt)

Run the code in ISLR Lab 3.6.3. Put each chunk from the textbook in its own chunk. (Note that you will have to install the `car` package.)

Briefly explain what the `vif()` and `update()` functions do when applied to a linear model.

## Part b (Code: 0.5 pts; Explanation: 1 pt)

Jumping straight into modeling without looking at the data is a very bad idea. Create a scatterplot matrix showing only the three variables in the first `lm.fit` object (`medv`, `lstat`, `age`).

Do you see any evidence of nonlinearity? Any evidence of collinearity? Explain your reasoning.
