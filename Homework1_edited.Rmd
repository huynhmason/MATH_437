---
title: 'Homework Assignment #1'
author: "Math 437 - Modern Data Analysis"
date: "Due February 10, 2023"
output: pdf_document
---

# Instructions

You should submit either two or three files:

1. You should write your solutions to the Simulation and Applied Problems in this R Markdown file and submit the (.Rmd) file.
2. You should knit the final solution file to pdf and submit the pdf. If you are having trouble getting code chunks to run, add `eval = FALSE` to the chunks that do not run. If you are having trouble getting R Studio to play nice with your LaTeX distribution, I will begrudgingly accept an HTML file instead. 
3. Solutions to the Key Terms and Conceptual Problems can be submitted in a separate Word or pdf file or included in the same files as your solutions to the Simulation and Applied Problems.

This homework assignment is worth a total of **35 points**.

# Key Terms (5 pts)

Read Chapter 2 of Introduction to Statistical Learning, Second Edition. Based on your reading, answer the following questions.

1. What is the difference between an *input variable* and an *output variable* in a model? Provide synonyms for each term.

An "input variable" is also known as predictors, independent variables, or features and is usually denoted by the letter "X," sometimes with a subscript. An "output variable" is also known as the "response" or "dependent variable" and is usually denoted by the letter "Y."

2. What is the difference between *reducible error* and *irreducible error*? Give an example (other than those given in the book) of a situation in which the irreducible error is greater than zero.

"Reducible error" can be improved by changing the accuracy using the most appropriate statistical learning technique to measure f. "Irreducible error" cannot be changed no matter how accurate the measure for f is, hence it being called 'irreducible.' For an example in which irreducible error is greater than zero, a basketball player's free-throw accuracy might vary, depending on their distance from the free-throw line or the pressure they are feeling from the game. 

3. Generally, what types of questions are answered using *inference* and what types are answered using *prediction*? Is it possible to use the same model for both inference and prediction?

Types of questions that can be answered with *inference* include finding the predictor values and examining the relationship between the predictor and response variables. Types of questions that can be responded using *predictions* are questions that are looking for actual values based on our data. It is possible to use the same model for both inference and prediction. We should know that some models are better for interpreting *inference* than *prediction* and vice versa. 

4. Generally, what types of prediction questions are answered using *regression* methods and what types are answered using *classification* methods?

Quantitative prediction questions, taking on numerical values, are generally answered using *regression* methods and qualitative prediction questions, utilizing categories, are answered using *classification* methods. 

5. What are the major advantages of using a *nonparametric* method over a *parametric* method? What are the disadvantages?

*Nonparametric* methods seek an estimate of f to get as close to the actual point as possible. This implies that these methods can be used on a wider range of shapes of f.

6. In prediction, we typically aim to minimize a *loss function* that more-or-less represents the total error in our predictions. Give one example each for regression and classification problems of a measure of model (in)accuracy.

*Mean squared error* is an example of a loss function for regression problems. *Training error rate* is an example of a loss function for classification problems. 

7. Why do we only fit the model on a *training set*? What do we do with the rest of the data?

We fit the model on a *training set* to build the model. The rest of the data is used to evaluate how good the *training set* is. 

8. Generally, as a model becomes more complex, what happens to the *bias* of the model and why? What happens to the *variance* of the model and why?

As a model becomes more complex, the *bias* of the model will decrease and the *variance* of the model will increase The *bias* increases, because more simple models have higher chances of error, which is what *bias* is. Complex models will be more *variable* because smaller changes in the data lead to greater changes in f_hat.

9. What is meant by the term *overfitting*? Explain this in terms of the bias-variance trade-off.

*Overfitting* means that the data follows the errors closely. In terms of bias-variance, a model that *overfits* the data has higher variance and lower bias and vice versa. 

10. Briefly explain how a *Bayes classifier* works.

A *Bayes classifier* is the conditional probability that is used to produce the lowest possible error rate called the *Bayes error rule.* The conditional probability for *Bayes classifier* is P(Y = j | X = x0), where j = 1, 2 depending on which response variable it is referring to and x0 is the predictor value..... 

(I think this answers the question, but I didn't want to delete your answer)
A Bayes classifier assigns each observation to the most likely class, given its predictor values. The Bayes classifier will always choose the class for which the conditional probability is largest. 

# Conceptual Problems

## Conceptual Problem 1 (4 pts)

Write me a brief (2-3 paragraphs) summary of what you learned in the P-Values and Power in-class activity about how the distribution of p-values (over very many tests) is affected by the validity/violation of test assumptions and the power of the test. Did anything surprise you or clarify a concept for you? Support your writing with a few graphs you produced in class (it is easiest to copy and re-run the relevant code chunks).

###

## Conceptual Problem 2 (3 pts) 

Textbook Exercise 2.4.4

*a* Classification might be useful in everyday life when I am classifying between a man and a woman, whether the weather is going to be cold or not cold, and classifying ... The response variables for comparing male and female would be hair, clothing, and mannerisms to name a few. Based on these variables, I believe the goal of this application is prediction because we are using the response variable to classify whether or not the person is male or female. The response variables for classifying the weather is the temperature, if the sun is out and if there are clouds. Using these response variables, we predict whether it will be cold, rainy, warm, or hot. 
#####

*b* Regression can be used in everyday life when comparing sales for a company, predicting the stock price, and when we go shopping for clothes. Comparing sales for a company can be something as simple as using the sales from the previous week to predict the following week or more complex in using sales from the previous year to predict sales for this year. The same thing can be used when determining whether or not stock prices could go up or go down in value between today and tomorrow based on previous days. When you go shopping, you can predict how expensive something is based on response variables such as the company you are shopping from, the material and quality, as well as the size of the clothing item. 

*c* Some real-life applications of cluster analysis include streaming services where they can collect data to see which areas have lower usage users and focus more on advertisements in that area, health insurance where companies could determine their monthly premiums based on the number of doctor visits a year, household size, and average age in the household along with other variables, and earthquake studies where researchers can cluster different areas based on whether or not they are on fault lines 

## Conceptual Problem 3 (3 pts) 

Textbook Exercise 13.7.2

(a) Bernoulli with probability alpha

(b) Binomial with m trials and probability alpha

(c) sqrt(m*alpha*(1-alpha))


# Simulation Problems

## Simulation Problem 1 (Code: 1.5 pts; Explanation: 3.5 pts)

From the Parametric vs. Nonparametric Tests: Two-Sample Tests activity, copy to this homework your simulation code/results from the *Assumptions Violated, Ha True* section of each test as well as the results tables for all simulations (in the Class Results section). 
Write a couple of paragraphs explaining the difference between parametric and nonparametric methods and describing under what conditions we might prefer to use a classic nonparametric method (Mann-Whitney) Instead of the corresponding parametric method (two-sample t-test).

######
```{r}
pvalues <- numeric(length = 10000)

nG <- 50
for (i in 1:length(pvalues)){
  set.seed(i)  # notice that the seed changes every time inside the for loop
  # you could also set a single seed outside the for loop
  
  # Create the vectors x and y
  x <- c(rnorm(nG*.9, mean = 0, sd = sqrt(0.19)), rnorm(nG*.1, mean = 3, sd = sqrt(0.19)))
  y <- c(rnorm(nG*.9, mean = 0.8, sd = sqrt(0.19)), rnorm(nG*.1, mean = 3.8, sd = sqrt(0.19)))

  # Perform the t-test and get the p-value
  ttest = t.test(x, y, alternative = "t")
  pvalues[i] <- ttest$p.value
    
}
```
```{r}
hist(pvalues)

plot(ecdf(pvalues),
     xlab = "p-value",
     ylab = "F(p-value)",
     main = "Empirical CDF of the P-Value Under H0")

mean(pvalues <= 0.05)
```
Parametric methods are utilized when a variable is assumed to be normally distributed and there are no outliers.  Parametric methods are used with data that sufficiently fits a distribution. They can also estimate the value of a point when there are no data.
  Nonparametric methods seek an estimate of f to get as close to the actual point as possible. This implies that these methods can be used on a wider range of shapes of f. These methods are used when outliers in the data cannot be removed or when a distribution cannot be applied to the dataset. 
  Based on the Parametric vs. Nonparametric activity we did in class, increasing power led to a lower probability of making a type 2 error.

# Applied Problems

## Applied Problem 1 (Code: 6 pts; Explanation: 3 pts)

Textbook Exercise 2.4.8 with the following changes:

* Use the `College` dataset already in the `ISLR2` package instead of doing parts (a) and (b).
* Replace the four lines of code in part (c.iv) with a single line that accomplishes the same thing, using the `mutate` and either `if_else` or `case_when` functions from the `dplyr` package.
* As part of your brief summary in part (c.vi), identify at least one data point that cannot possibly have been recorded correctly, and explain why.

```{r 2.4.8 part a and b}
coll <- ISLR2::College
```

```{r 2.4.8 part c}
# c.i
summary(coll)

# c.ii
A <- coll[ , 1:10]
pairs(A)

# c.iii
coll$Outstate
coll$Private <- as.factor(coll$Private)
boxplot(coll$Private, coll$Outstate)
#idk if the boxplot is correct
# c.iv
Elite <- rep ( " No " , nrow ( coll ) )
Elite [ coll $ Top10perc > 50] <- " Yes "
Elite <- as.factor ( Elite )
coll <- data.frame ( coll , Elite )
summary(Elite)
boxplot(coll$Elite,coll$Outstate)
#I still don't think the boxplot is correct
# c.v


# c.vi

```


## Applied Problem 2 (Code: 1 pt; Explanation: 2 pts)

Molitor (1989) hypothesized that children who watched violent film and television were more tolerant of violent "real-life" behavior. A sample of 42 children were randomly assigned to watch footage from either the 1984 Summer Olympics (non-violent) or the movie \emph{The Karate Kid} (violent). They were then told to watch (by video monitor) two younger children in the next room and get the research assistant if they "got into trouble" (the monitor actually showed a pre-recorded video of the children getting progressively more violent).

The file \emph{violence.csv} contains the time (in seconds) that each child stayed in the room. Longer stays are assumed to indicate more tolerance of violent behavior. Produce an appropriate graph showing the sample data and, based on your graph, explain why a two-sample t-test might not be the best idea.

```{r}
data = read.csv("violence.csv", h = T)
View(data)
hist(data$Time)
```
In order to do a two sample t-test, the data must be normally distributed, but this dataset has an outlier and skewed right, as seen by the boxplot.

## Applied Problem 3 (Code: 1 pt; Explanation: 2 pts)

Use the permutation test function you wrote in Lab 2 to determine whether the research hypothesis in the previous question was supported. Be sure to follow all steps of hypothesis testing, up to and including writing a conclusion that answers the research question in context.
